{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"A true friend loves you unconditionally, understands you, but never judges you and always tries to support you and give you good advice.\n",
    "The friendship of Krishna and Sudama is a great example of true friendship.\n",
    "A true friend is one who will always be there when you need someone.\n",
    "He will leave all his important work, but will never leave you alone, especially in your difficult times.\n",
    "That is why it is said a friend in need is a friend indeed.\n",
    "Difficult times are the best time to realize who your true friends are.\n",
    "Blessed are the souls who have true friends. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A true friend loves you unconditionally, understands you, but never judges you and always tries to support you and give you good advice.', 'The friendship of Krishna and Sudama is a great example of true friendship.', 'A true friend is one who will always be there when you need someone.', 'He will leave all his important work, but will never leave you alone, especially in your difficult times.', 'That is why it is said a friend in need is a friend indeed.', 'Difficult times are the best time to realize who your true friends are.', 'Blessed are the souls who have true friends.'] "
     ]
    }
   ],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "print(sentences,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'true', 'friend', 'loves', 'you', 'unconditionally', ',', 'understands', 'you', ',', 'but', 'never', 'judges', 'you', 'and', 'always', 'tries', 'to', 'support', 'you', 'and', 'give', 'you', 'good', 'advice', '.', 'The', 'friendship', 'of', 'Krishna', 'and', 'Sudama', 'is', 'a', 'great', 'example', 'of', 'true', 'friendship', '.', 'A', 'true', 'friend', 'is', 'one', 'who', 'will', 'always', 'be', 'there', 'when', 'you', 'need', 'someone', '.', 'He', 'will', 'leave', 'all', 'his', 'important', 'work', ',', 'but', 'will', 'never', 'leave', 'you', 'alone', ',', 'especially', 'in', 'your', 'difficult', 'times', '.', 'That', 'is', 'why', 'it', 'is', 'said', 'a', 'friend', 'in', 'need', 'is', 'a', 'friend', 'indeed', '.', 'Difficult', 'times', 'are', 'the', 'best', 'time', 'to', 'realize', 'who', 'your', 'true', 'friends', 'are', '.', 'Blessed', 'are', 'the', 'souls', 'who', 'have', 'true', 'friends', '.']\n"
     ]
    }
   ],
   "source": [
    "words=nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('true', 'JJ'), ('friend', 'NN'), ('loves', 'VBZ'), ('unconditionally', 'RB'), (',', ','), ('understands', 'NNS'), (',', ','), ('never', 'RB'), ('judges', 'NNS'), ('always', 'RB'), ('tries', 'VBZ'), ('support', 'NN'), ('give', 'VB'), ('good', 'JJ'), ('advice', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('friendship', 'NN'), ('Krishna', 'NNP'), ('Sudama', 'NNP'), ('great', 'JJ'), ('example', 'NN'), ('true', 'JJ'), ('friendship', 'NN'), ('.', '.')]\n",
      "[('A', 'DT'), ('true', 'JJ'), ('friend', 'NN'), ('one', 'CD'), ('always', 'RB'), ('need', 'VB'), ('someone', 'NN'), ('.', '.')]\n",
      "[('He', 'PRP'), ('leave', 'VBP'), ('important', 'JJ'), ('work', 'NN'), (',', ','), ('never', 'RB'), ('leave', 'VBP'), ('alone', 'RB'), (',', ','), ('especially', 'RB'), ('difficult', 'JJ'), ('times', 'NNS'), ('.', '.')]\n",
      "[('That', 'DT'), ('said', 'VBD'), ('friend', 'VBP'), ('need', 'MD'), ('friend', 'VB'), ('indeed', 'RB'), ('.', '.')]\n",
      "[('Difficult', 'NNP'), ('times', 'NNS'), ('best', 'JJS'), ('time', 'NN'), ('realize', 'VB'), ('true', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Blessed', 'VBN'), ('souls', 'NNS'), ('true', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "    wordsList = [w for w in wordsList if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A true friend loves unconditionally , understands , never judges always tries support give good advice .', 'The friendship Krishna Sudama great example true friendship .', 'A true friend one always need someone .', 'He leave important work , never leave alone , especially difficult times .', 'That said friend need friend indeed .', 'Difficult times best time realize true friends .', 'Blessed souls true friends .']\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A true friend love uncondit , understand , never judg alway tri support give good advic .', 'the friendship krishna sudama great exampl true friendship .', 'A true friend one alway need someon .', 'He leav import work , never leav alon , especi difficult time .', 'that said friend need friend inde .', 'difficult time best time realiz true friend .', 'bless soul true friend .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "for i in range (len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A true friend love unconditionally , understands , never judge always try support give good advice .', 'The friendship Krishna Sudama great example true friendship .', 'A true friend one always need someone .', 'He leave important work , never leave alone , especially difficult time .', 'That said friend need friend indeed .', 'Difficult time best time realize true friend .', 'Blessed soul true friend .']\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "for i in range (len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wordnet.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30205477 0.         0.2507314  0.         0.         0.\n",
      "  0.         0.         0.16299352 0.         0.30205477 0.30205477\n",
      "  0.         0.         0.         0.30205477 0.         0.\n",
      "  0.30205477 0.         0.2507314  0.         0.         0.\n",
      "  0.         0.         0.         0.30205477 0.         0.16299352\n",
      "  0.30205477 0.30205477 0.30205477 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.34728953 0.         0.69457906 0.         0.\n",
      "  0.34728953 0.         0.         0.         0.34728953 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.34728953 0.         0.         0.18740291\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.41710986 0.         0.         0.\n",
      "  0.         0.         0.27115153 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.41710986 0.         0.50249001 0.         0.\n",
      "  0.50249001 0.         0.         0.         0.         0.27115153\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.3151717  0.         0.         0.         0.26161958\n",
      "  0.3151717  0.         0.         0.         0.         0.\n",
      "  0.         0.3151717  0.         0.         0.         0.6303434\n",
      "  0.         0.         0.26161958 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.26161958 0.\n",
      "  0.         0.         0.         0.3151717 ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.54975728 0.         0.         0.\n",
      "  0.         0.         0.50939697 0.         0.         0.\n",
      "  0.         0.42284323 0.         0.         0.         0.50939697\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.40731311 0.         0.33810486\n",
      "  0.         0.         0.21979258 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.40731311 0.\n",
      "  0.         0.         0.         0.         0.67620973 0.21979258\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.62228701 0.\n",
      "  0.         0.         0.33579589 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.62228701 0.         0.         0.         0.33579589\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
